"""
æ–‡ä»¶input(ä¾èµ–å¤–éƒ¨ä»€ä¹ˆ): results/alpha_profit_employee/dynamic_backtest_20260106_230006/alpha_profit_employee_factor_dynamic_20250101_20251231.csv
æ–‡ä»¶output(æä¾›ä»€ä¹ˆ): æˆªé¢æ ·æœ¬é‡åˆ†å¸ƒç»Ÿè®¡æŠ¥å‘Š, åŒ…å«å„æ—¥æœŸè‚¡ç¥¨æ•°é‡åˆ†å¸ƒã€ç»Ÿè®¡ä¿¡æ¯ã€é—®é¢˜åˆ†æ
æ–‡ä»¶pos(ç³»ç»Ÿå±€éƒ¨åœ°ä½): å› å­åˆ†æå·¥å…·, ç”¨äºè¯Šæ–­alpha_profit_employeeå› å­çš„æˆªé¢æ ·æœ¬é‡ä¸å‡è¡¡é—®é¢˜

è¯¦ç»†è¯´æ˜:
1. åŠ è½½åŠ¨æ€æˆªé¢å›æµ‹ç”Ÿæˆçš„å› å­æ•°æ®
2. ç»Ÿè®¡æ¯ä¸ªäº¤æ˜“æ—¥çš„å¯ç”¨è‚¡ç¥¨æ•°é‡
3. åˆ†ææˆªé¢æ ·æœ¬é‡åˆ†å¸ƒç‰¹å¾
4. è¯†åˆ«å°æˆªé¢é—®é¢˜
5. æä¾›ä¼˜åŒ–å»ºè®®

ä½¿ç”¨ç¤ºä¾‹:
    python3 scripts/analysis/analyze_cross_section_size.py

è¿”å›å€¼:
    ç”Ÿæˆæˆªé¢æ ·æœ¬é‡åˆ†ææŠ¥å‘Šåˆ° results/alpha_profit_employee/ ç›®å½•
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import os

def load_factor_data():
    """åŠ è½½å› å­æ•°æ®"""
    result_dir = "/home/zcy/alphaå› å­åº“/results/alpha_profit_employee/dynamic_backtest_20260106_230006"
    factor_file = os.path.join(result_dir, "alpha_profit_employee_factor_dynamic_20250101_20251231.csv")

    if not os.path.exists(factor_file):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°å› å­æ–‡ä»¶ {factor_file}")
        return None

    df = pd.read_csv(factor_file)
    print(f"åŠ è½½å› å­æ•°æ®: {len(df)} æ¡è®°å½•")
    print(f"æ—¥æœŸèŒƒå›´: {df['trade_date'].min()} ~ {df['trade_date'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {df['ts_code'].nunique()}")

    return df

def analyze_cross_section_size(df):
    """åˆ†ææˆªé¢æ ·æœ¬é‡åˆ†å¸ƒ"""
    print("\n" + "="*60)
    print("æˆªé¢æ ·æœ¬é‡åˆ†å¸ƒåˆ†æ")
    print("="*60)

    # æŒ‰äº¤æ˜“æ—¥ç»Ÿè®¡è‚¡ç¥¨æ•°é‡
    daily_counts = df.groupby('trade_date')['ts_code'].nunique().sort_values()

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'æ€»äº¤æ˜“æ—¥æ•°': len(daily_counts),
        'å¹³å‡æ¯æ—¥è‚¡ç¥¨æ•°': daily_counts.mean(),
        'ä¸­ä½æ•°': daily_counts.median(),
        'æœ€å°å€¼': daily_counts.min(),
        'æœ€å¤§å€¼': daily_counts.max(),
        'æ ‡å‡†å·®': daily_counts.std(),
    }

    print("\nåŸºç¡€ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value:.2f}")

    # åˆ†ç»„ç»Ÿè®¡
    bins = [0, 5, 10, 20, 50, 100, 500, 1000]
    labels = ['1-4åª', '5-9åª', '10-19åª', '20-49åª', '50-99åª', '100-499åª', '500+åª']
    daily_counts_grouped = pd.cut(daily_counts, bins=bins, labels=labels, right=False)
    distribution = daily_counts_grouped.value_counts().sort_index()

    print("\næˆªé¢å¤§å°åˆ†å¸ƒ:")
    for label, count in distribution.items():
        percentage = count / len(daily_counts) * 100
        print(f"  {label}: {count}å¤© ({percentage:.1f}%)")

    # è¯†åˆ«é—®é¢˜æ—¥æœŸ
    small_cross_section_dates = daily_counts[daily_counts < 5]
    medium_cross_section_dates = daily_counts[(daily_counts >= 5) & (daily_counts < 10)]

    print(f"\nâš ï¸  é—®é¢˜æˆªé¢:")
    print(f"  å°æˆªé¢(<5åª): {len(small_cross_section_dates)}å¤© ({len(small_cross_section_dates)/len(daily_counts)*100:.1f}%)")
    if len(small_cross_section_dates) > 0:
        print(f"    æœ€å°å€¼: {small_cross_section_dates.min()}åª")
        print(f"    æ—¥æœŸç¤ºä¾‹: {small_cross_section_dates.index[:5].tolist()}")

    print(f"  ä¸­ç­‰æˆªé¢(5-9åª): {len(medium_cross_section_dates)}å¤© ({len(medium_cross_section_dates)/len(daily_counts)*100:.1f}%)")

    return daily_counts, distribution

def analyze_factor_value_distribution_by_size(df, daily_counts):
    """åˆ†æä¸åŒæˆªé¢å¤§å°ä¸‹çš„å› å­å€¼åˆ†å¸ƒ"""
    print("\n" + "="*60)
    print("å› å­å€¼ä¸æˆªé¢å¤§å°å…³ç³»åˆ†æ")
    print("="*60)

    # æ·»åŠ æˆªé¢å¤§å°ä¿¡æ¯
    df_analysis = df.copy()
    df_analysis['cross_section_size'] = df_analysis['trade_date'].map(daily_counts)

    # æŒ‰æˆªé¢å¤§å°åˆ†ç»„ç»Ÿè®¡å› å­å€¼
    def size_group(size):
        if size < 5:
            return 'å°æˆªé¢(<5)'
        elif size < 10:
            return 'ä¸­æˆªé¢(5-9)'
        elif size < 20:
            return 'è¾ƒå¤§æˆªé¢(10-19)'
        else:
            return 'å¤§æˆªé¢(20+)'

    df_analysis['size_group'] = df_analysis['cross_section_size'].apply(size_group)

    # ç»Ÿè®¡å„ç»„çš„å› å­å€¼ç‰¹å¾
    grouped_stats = df_analysis.groupby('size_group')['factor'].agg([
        'count', 'mean', 'std', 'min', 'max'
    ]).round(4)

    print("\nä¸åŒæˆªé¢å¤§å°çš„å› å­å€¼ç»Ÿè®¡:")
    print(grouped_stats)

    # åˆ†æå°æˆªé¢çš„å› å­å€¼åˆ†å¸ƒ
    small_sections = df_analysis[df_analysis['cross_section_size'] < 5]
    if len(small_sections) > 0:
        print(f"\nâš ï¸  å°æˆªé¢(<5åª)è¯¦ç»†åˆ†æ:")
        print(f"  è®°å½•æ•°: {len(small_sections)}")
        print(f"  å› å­å€¼èŒƒå›´: [{small_sections['factor'].min():.4f}, {small_sections['factor'].max():.4f}]")
        print(f"  å› å­å€¼å‡å€¼: {small_sections['factor'].mean():.4f}")

        # æŸ¥çœ‹å°æˆªé¢ä¸­å› å­å€¼ä¸º1.0çš„æ¯”ä¾‹
        max_factor_ratio = (small_sections['factor'] == 1.0).sum() / len(small_sections) * 100
        print(f"  å› å­å€¼=1.0çš„æ¯”ä¾‹: {max_factor_ratio:.1f}%")

        # æŸ¥çœ‹å°æˆªé¢æ—¥æœŸåˆ†å¸ƒ
        small_date_counts = small_sections.groupby('trade_date').size()
        print(f"  æ¶‰åŠäº¤æ˜“æ—¥æ•°: {len(small_date_counts)}")
        print(f"  æ¯æ—¥å°æˆªé¢è‚¡ç¥¨æ•°åˆ†å¸ƒ: {small_date_counts.value_counts().sort_index().to_dict()}")

    return df_analysis

def calculate_impact_on_backtest(df, daily_counts):
    """è®¡ç®—æˆªé¢æ ·æœ¬é‡ä¸å‡è¡¡å¯¹å›æµ‹çš„å½±å“"""
    print("\n" + "="*60)
    print("æˆªé¢æ ·æœ¬é‡ä¸å‡è¡¡å¯¹å›æµ‹çš„å½±å“åˆ†æ")
    print("="*60)

    # æ·»åŠ æˆªé¢å¤§å°ä¿¡æ¯
    df_impact = df.copy()
    df_impact['cross_section_size'] = df_impact['trade_date'].map(daily_counts)

    # åˆ†æå°æˆªé¢æ—¥æœŸçš„å› å­è¡¨ç°
    small_section_dates = daily_counts[daily_counts < 5].index
    normal_section_dates = daily_counts[daily_counts >= 5].index

    small_section_data = df_impact[df_impact['trade_date'].isin(small_section_dates)]
    normal_section_data = df_impact[df_impact['trade_date'].isin(normal_section_dates)]

    print(f"\nå°æˆªé¢æ—¥æœŸ({len(small_section_dates)}å¤©) vs æ­£å¸¸æˆªé¢æ—¥æœŸ({len(normal_section_dates)}å¤©):")

    if len(small_section_data) > 0:
        print(f"\nå°æˆªé¢æ—¥æœŸ:")
        print(f"  è®°å½•æ•°: {len(small_section_data)}")
        print(f"  å› å­å‡å€¼: {small_section_data['factor'].mean():.4f}")
        print(f"  å› å­æ ‡å‡†å·®: {small_section_data['factor'].std():.4f}")
        print(f"  å› å­å€¼=1.0çš„æ¯”ä¾‹: {(small_section_data['factor'] == 1.0).sum() / len(small_section_data) * 100:.1f}%")

    if len(normal_section_data) > 0:
        print(f"\næ­£å¸¸æˆªé¢æ—¥æœŸ:")
        print(f"  è®°å½•æ•°: {len(normal_section_data)}")
        print(f"  å› å­å‡å€¼: {normal_section_data['factor'].mean():.4f}")
        print(f"  å› å­æ ‡å‡†å·®: {normal_section_data['factor'].std():.4f}")
        print(f"  å› å­å€¼=1.0çš„æ¯”ä¾‹: {(normal_section_data['factor'] == 1.0).sum() / len(normal_section_data) * 100:.1f}%")

    # é‡åŒ–å½±å“
    if len(small_section_data) > 0 and len(normal_section_data) > 0:
        mean_diff = abs(small_section_data['factor'].mean() - normal_section_data['factor'].mean())
        print(f"\nğŸ“Š å½±å“é‡åŒ–:")
        print(f"  å‡å€¼å·®å¼‚: {mean_diff:.4f}")
        print(f"  å°æˆªé¢å› å­å€¼=1.0çš„æ¯”ä¾‹æ›´é«˜: {(small_section_data['factor'] == 1.0).sum() / len(small_section_data) * 100:.1f}% vs {(normal_section_data['factor'] == 1.0).sum() / len(normal_section_data) * 100:.1f}%")
        print(f"  å°æˆªé¢å› å­å€¼åˆ†å¸ƒæ›´é›†ä¸­: æ ‡å‡†å·® {small_section_data['factor'].std():.4f} vs {normal_section_data['factor'].std():.4f}")

def generate_recommendations(daily_counts):
    """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
    print("\n" + "="*60)
    print("ä¼˜åŒ–å»ºè®®")
    print("="*60)

    small_ratio = (daily_counts < 5).sum() / len(daily_counts) * 100
    medium_ratio = ((daily_counts >= 5) & (daily_counts < 10)).sum() / len(daily_counts) * 100

    print("\n1. æœ€å°æ ·æœ¬é‡è¿‡æ»¤")
    print(f"   - é—®é¢˜: {small_ratio:.1f}%çš„äº¤æ˜“æ—¥æˆªé¢æ ·æœ¬é‡<5åª")
    print(f"   - å»ºè®®: è¿‡æ»¤æ‰æ ·æœ¬é‡<5çš„æˆªé¢ï¼Œä¸å‚ä¸å½“æ—¥é€‰è‚¡")
    print(f"   - é¢„æœŸå½±å“: {small_ratio:.1f}%çš„äº¤æ˜“æ—¥å¯èƒ½æ²¡æœ‰è‚¡ç¥¨å¯é€‰")

    print("\n2. ä¸­ç­‰æ ·æœ¬é‡å¹³æ»‘å¤„ç†")
    print(f"   - é—®é¢˜: {medium_ratio:.1f}%çš„äº¤æ˜“æ—¥æˆªé¢æ ·æœ¬é‡åœ¨5-9åªä¹‹é—´")
    print(f"   - å»ºè®®: å¯¹è¿™äº›æˆªé¢ä½¿ç”¨åŠ æƒæ’åï¼Œé™ä½å°æˆªé¢å› å­å€¼çš„æƒé‡")
    print(f"   - å®ç°: factor = raw_rank * (n/10) + 0.5 * (1 - n/10)")

    print("\n3. å› å­æ–¹å‘è°ƒæ•´")
    print(f"   - å½“å‰é—®é¢˜: é«˜å› å­å€¼ç»„æ”¶ç›Šåä½")
    print(f"   - å»ºè®®: å°è¯•ä½¿ç”¨ -alpha_profit_employee")

    print("\n4. è¡Œä¸šä¸­æ€§åŒ–")
    print(f"   - é—®é¢˜: ä¸åŒè¡Œä¸šçš„åˆ©æ¶¦ç»“æ„å·®å¼‚å¤§")
    print(f"   - å»ºè®®: å‡å»è¡Œä¸šå‡å€¼ï¼Œæ¶ˆé™¤è¡Œä¸šåå·®")

    print("\n5. å¸‚å€¼ä¸­æ€§åŒ–")
    print(f"   - é—®é¢˜: å¤§å¸‚å€¼å…¬å¸å¯èƒ½å› å­å€¼é«˜ä½†å¢é•¿æ…¢")
    print(f"   - å»ºè®®: å…ˆæŒ‰å¸‚å€¼åˆ†ç»„ï¼Œå†ç»„å†…æ’å")

def main():
    """ä¸»å‡½æ•°"""
    print("Alpha Profit Employeeå› å­ - æˆªé¢æ ·æœ¬é‡ä¸å‡è¡¡é—®é¢˜åˆ†æ")
    print("="*60)

    # 1. åŠ è½½æ•°æ®
    df = load_factor_data()
    if df is None:
        return

    # 2. åˆ†ææˆªé¢æ ·æœ¬é‡åˆ†å¸ƒ
    daily_counts, distribution = analyze_cross_section_size(df)

    # 3. åˆ†æå› å­å€¼ä¸æˆªé¢å¤§å°çš„å…³ç³»
    df_analysis = analyze_factor_value_distribution_by_size(df, daily_counts)

    # 4. è®¡ç®—å¯¹å›æµ‹çš„å½±å“
    calculate_impact_on_backtest(df, daily_counts)

    # 5. ç”Ÿæˆä¼˜åŒ–å»ºè®®
    generate_recommendations(daily_counts)

    # 6. ä¿å­˜æŠ¥å‘Š
    result_dir = "/home/zcy/alphaå› å­åº“/results/alpha_profit_employee"
    output_file = os.path.join(result_dir, f"cross_section_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

    print(f"\n" + "="*60)
    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    print("="*60)

if __name__ == "__main__":
    main()